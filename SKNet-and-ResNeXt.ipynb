{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "\n",
        "Before running, put the files data_batch_1, data_batch_2, data_batch_3, data_batch_4, data_batch_5, test_batch in the root directory. \n",
        "\n",
        "In our experiment, epoch_num = 100, batch_size=100."
      ],
      "metadata": {
        "id": "2yOrgOkC6Az8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CIFAR-10"
      ],
      "metadata": {
        "id": "md_fAF-yQRNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from keras.datasets import cifar10\n",
        "import time\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, mode='train', root_path='/'):\n",
        "        super(MyDataset, self).__init__()\n",
        "        if mode == 'train':\n",
        "            file_path = os.path.join(root_path, 'data_batch_{}')\n",
        "            self.data, self.labels = load_traindata(file_path=file_path)\n",
        "        elif mode == 'test':\n",
        "            file_path = os.path.join(root_path, 'test_batch')\n",
        "            data_dict = unpickle(file_path)\n",
        "            self.data = data_dict[b'data']\n",
        "            self.labels = data_dict[b'labels']\n",
        "        self.data = self.data/255\n",
        "        self.num = len(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index, :].reshape(3, 32, 32).astype(np.float32), self.labels[index]\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "def load_traindata(file_path):\n",
        "    train_data = None\n",
        "    train_labels = None\n",
        "    if not os.path.exists(file_path.format(1)):\n",
        "        print('wrong dataset path : {}'.format(file_path.format(1)))\n",
        "        exit()\n",
        "    for i in range(5):\n",
        "        data_dict = unpickle(file_path.format(i+1))\n",
        "        if train_data is None:\n",
        "            train_data = data_dict[b'data']\n",
        "            train_labels = data_dict[b'labels']\n",
        "        else:\n",
        "            train_data = np.concatenate((train_data, data_dict[b'data']), axis=0)\n",
        "            train_labels = np.concatenate((train_labels, data_dict[b'labels']), axis=0)\n",
        "    return train_data, train_labels"
      ],
      "metadata": {
        "id": "ToSni8zGtu8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNeXt"
      ],
      "metadata": {
        "id": "vdL1_btb6PLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNeXtUnit(nn.Module):\n",
        "    def __init__(self, in_features, out_features, mid_features=None, stride=1, groups=32):\n",
        "        super(ResNeXtUnit, self).__init__()\n",
        "        if mid_features is None:\n",
        "            mid_features = int(out_features/2)\n",
        "        self.feas = nn.Sequential(\n",
        "            nn.Conv2d(in_features, mid_features, 1, stride=1),\n",
        "            nn.BatchNorm2d(mid_features),\n",
        "            nn.Conv2d(mid_features, mid_features, 3, stride=stride, padding=1, groups=groups),\n",
        "            nn.BatchNorm2d(mid_features),\n",
        "            nn.Conv2d(mid_features, out_features, 1, stride=1),\n",
        "            nn.BatchNorm2d(out_features)\n",
        "        )\n",
        "        if in_features == out_features:\n",
        "            self.shortcut = nn.Sequential()\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_features, out_features, 1, stride=stride),\n",
        "                nn.BatchNorm2d(out_features)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        fea = self.feas(x)\n",
        "        return fea + self.shortcut(x)\n",
        "\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, class_num):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.basic_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ) # 32x32\n",
        "        self.stage_1 = nn.Sequential(\n",
        "            ResNeXtUnit(64, 256, mid_features=128),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(256, 256),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(256, 256),\n",
        "            nn.ReLU()\n",
        "        ) # 32x32\n",
        "        self.stage_2 = nn.Sequential(\n",
        "            ResNeXtUnit(256, 512, stride=2),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(512, 512),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(512, 512),\n",
        "            nn.ReLU()\n",
        "        ) # 16x16\n",
        "        self.stage_3 = nn.Sequential(\n",
        "            ResNeXtUnit(512, 1024, stride=2),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            ResNeXtUnit(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        ) # 8x8\n",
        "        self.pool = nn.AvgPool2d(8)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, class_num),\n",
        "            #nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        fea = self.basic_conv(x)\n",
        "        fea = self.stage_1(fea)\n",
        "        fea = self.stage_2(fea)\n",
        "        fea = self.stage_3(fea)\n",
        "        fea = self.pool(fea)\n",
        "        fea = torch.squeeze(fea) #torch.Size([1024])\n",
        "        fea = self.classifier(fea) #torch.Size([10])\n",
        "        return fea\n",
        "\n",
        "\n",
        "    # x = torch.rand(1, 3, 32, 32)\n",
        "    # a = nn.Sequential(\n",
        "    #         nn.Conv2d(3, 64, 3, padding=1),\n",
        "    #         nn.BatchNorm2d(64)\n",
        "    #     )(x)\n",
        "    # b = nn.Sequential(\n",
        "    #         ResNeXtUnit(64, 256, mid_features=128),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(256, 256),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(256, 256),\n",
        "    #         nn.ReLU()\n",
        "    #     )(a)\n",
        "    # c = nn.Sequential(\n",
        "    #         ResNeXtUnit(256, 512, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(512, 512),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(512, 512),\n",
        "    #         nn.ReLU()\n",
        "    #     )(b)\n",
        "    # d = nn.Sequential(\n",
        "    #         ResNeXtUnit(512, 1024, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(1024, 1024),\n",
        "    #         nn.ReLU(),\n",
        "    #         ResNeXtUnit(1024, 1024),\n",
        "    #         nn.ReLU()\n",
        "    #     )(c)\n",
        "    # print(a.size())\n",
        "    # print(b.size())\n",
        "    # print(c.size())\n",
        "    # print(d.size())\n",
        "    # e=nn.AvgPool2d(8)(d).squeeze()\n",
        "    # print(e.size())\n",
        "    # f=nn.Linear(1024, 10)(e)\n",
        "    # print(f)"
      ],
      "metadata": {
        "id": "ewm_6AHGs0pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SKNet"
      ],
      "metadata": {
        "id": "ZayELZTS6XIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SKConv(nn.Module):\n",
        "    def __init__(self, features, WH, M, G, r, stride=1 ,L=32):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            features: input channel dimensionality.\n",
        "            WH: input spatial dimensionality, used for GAP kernel size.\n",
        "            M: the number of branchs.\n",
        "            G: num of convolution groups.\n",
        "            r: the radio for compute d, the length of z.\n",
        "            stride: stride, default 1.\n",
        "            L: the minimum dim of the vector z in paper, default 32.\n",
        "        \"\"\"\n",
        "        super(SKConv, self).__init__()\n",
        "        d = max(int(features/r), L)\n",
        "        self.M = M\n",
        "        self.features = features\n",
        "        self.convs = nn.ModuleList([])\n",
        "        for i in range(M):\n",
        "            self.convs.append(nn.Sequential(\n",
        "                nn.Conv2d(features, features, kernel_size=3+i*2, stride=stride, padding=1+i, groups=G),\n",
        "                nn.BatchNorm2d(features),\n",
        "                nn.ReLU(inplace=False)\n",
        "            ))\n",
        "        self.fc = nn.Linear(features, d)\n",
        "        self.fcs = nn.ModuleList([])\n",
        "        for i in range(M):\n",
        "            self.fcs.append(\n",
        "                nn.Linear(d, features)\n",
        "            )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            fea = torch.unsqueeze(conv(x),dim=1)\n",
        "            if i == 0:\n",
        "                feas = fea\n",
        "            else:\n",
        "                feas = torch.cat([feas, fea], dim=1)\n",
        "        fea_U = torch.sum(feas, dim=1)\n",
        "        fea_s = fea_U.mean(-1).mean(-1)\n",
        "        fea_z = self.fc(fea_s)\n",
        "        for i, fc in enumerate(self.fcs):\n",
        "            vector = torch.unsqueeze(fc(fea_z),dim=1)\n",
        "            if i == 0:\n",
        "                attention_vectors = vector\n",
        "            else:\n",
        "                attention_vectors = torch.cat([attention_vectors, vector], dim=1)\n",
        "        attention_vectors = self.softmax(attention_vectors)\n",
        "        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)\n",
        "        fea_v = (feas * attention_vectors).sum(dim=1)\n",
        "        return fea_v\n",
        "\n",
        "\n",
        "class SKUnit(nn.Module):\n",
        "    def __init__(self, in_features, out_features, WH, M, G, r, mid_features=None, stride=1, L=32):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            in_features: input channel dimensionality.\n",
        "            out_features: output channel dimensionality.\n",
        "            WH: input spatial dimensionality, used for GAP kernel size.\n",
        "            M: the number of branchs.\n",
        "            G: num of convolution groups.\n",
        "            r: the radio for compute d, the length of z.\n",
        "            mid_features: the channle dim of the middle conv with stride not 1, default out_features/2.\n",
        "            stride: stride.\n",
        "            L: the minimum dim of the vector z in paper.\n",
        "        \"\"\"\n",
        "        super(SKUnit, self).__init__()\n",
        "        if mid_features is None:\n",
        "            mid_features = int(out_features/2)\n",
        "        self.feas = nn.Sequential(\n",
        "            nn.Conv2d(in_features, mid_features, 1, stride=1),\n",
        "            nn.BatchNorm2d(mid_features),\n",
        "            SKConv(mid_features, WH, M, G, r, stride=stride, L=L),\n",
        "            nn.BatchNorm2d(mid_features),\n",
        "            nn.Conv2d(mid_features, out_features, 1, stride=1),\n",
        "            nn.BatchNorm2d(out_features)\n",
        "        )\n",
        "        if in_features == out_features:\n",
        "            self.shortcut = nn.Sequential()\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_features, out_features, 1, stride=stride),\n",
        "                nn.BatchNorm2d(out_features)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        fea = self.feas(x)\n",
        "        return fea + self.shortcut(x)\n",
        "\n",
        "\n",
        "class SKNet(nn.Module):\n",
        "    def __init__(self, class_num):\n",
        "        super(SKNet, self).__init__()\n",
        "        self.basic_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ) # 32x32\n",
        "        self.stage_1 = nn.Sequential(\n",
        "            SKUnit(64, 256, 32, 2, 8, 2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(256, 256, 32, 2, 8, 2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(256, 256, 32, 2, 8, 2),\n",
        "            nn.ReLU()\n",
        "        ) # 32x32\n",
        "        self.stage_2 = nn.Sequential(\n",
        "            SKUnit(256, 512, 32, 2, 8, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(512, 512, 32, 2, 8, 2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(512, 512, 32, 2, 8, 2),\n",
        "            nn.ReLU()\n",
        "        ) # 16x16\n",
        "        self.stage_3 = nn.Sequential(\n",
        "            SKUnit(512, 1024, 32, 2, 8, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(1024, 1024, 32, 2, 8, 2),\n",
        "            nn.ReLU(),\n",
        "            SKUnit(1024, 1024, 32, 2, 8, 2),\n",
        "            nn.ReLU()\n",
        "        ) # 8x8\n",
        "        self.pool = nn.AvgPool2d(8)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, class_num),\n",
        "            #nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.basic_conv(x)\n",
        "        fea = self.stage_1(fea)\n",
        "        fea = self.stage_2(fea)\n",
        "        fea = self.stage_3(fea)\n",
        "        fea = self.pool(fea)\n",
        "        fea = torch.squeeze(fea) #torch.Size([1024])\n",
        "        fea = self.classifier(fea) #torch.Size([10])\n",
        "        return fea\n",
        "\n",
        "\n",
        "    # x = torch.rand(1, 3, 32, 32)\n",
        "    # a = nn.Sequential(\n",
        "    #         nn.Conv2d(3, 64, 3, padding=1),\n",
        "    #         nn.BatchNorm2d(64)\n",
        "    #     )(x)\n",
        "    # b = nn.Sequential(\n",
        "    #         SKUnit(64, 256, 32, 2, 8, 2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(256, 256, 32, 2, 8, 2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(256, 256, 32, 2, 8, 2),\n",
        "    #         nn.ReLU()\n",
        "    #     )(a)\n",
        "    # c = nn.Sequential(\n",
        "    #         SKUnit(256, 512, 32, 2, 8, 2, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(512, 512, 32, 2, 8, 2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(512, 512, 32, 2, 8, 2),\n",
        "    #         nn.ReLU()\n",
        "    #     )(b)\n",
        "    # d = nn.Sequential(\n",
        "    #         SKUnit(512, 1024, 32, 2, 8, 2, stride=2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(1024, 1024, 32, 2, 8, 2),\n",
        "    #         nn.ReLU(),\n",
        "    #         SKUnit(1024, 1024, 32, 2, 8, 2),\n",
        "    #         nn.ReLU()\n",
        "    #     )(c)\n",
        "    # print(a.size())\n",
        "    # print(b.size())\n",
        "    # print(c.size())\n",
        "    # print(d.size())\n",
        "    # e=nn.AvgPool2d(8)(d).squeeze()\n",
        "    # print(e.size())\n",
        "    # f=nn.Linear(1024, 10)(e)\n",
        "    # print(f)"
      ],
      "metadata": {
        "id": "-HIyB2mCs3n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "Q7Ru_wNz6aMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, train_loader, criterion, epoch, writer=None):\n",
        "    model.train()\n",
        "    num = len(train_loader)\n",
        "    for i, (data, label) in enumerate(train_loader):\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        data = data.cuda()\n",
        "        label = label.cuda().long()\n",
        "        result = model(data)\n",
        "        loss = criterion(result, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i%10==0:\n",
        "            print('epoch {}, [{}/{}], loss {}'.format(epoch, i, num, loss))\n",
        "            if writer is not None:\n",
        "                writer.add_scalar('loss', loss.item(), epoch*num + i)\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, epoch, writer=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            data = data.cuda()\n",
        "            label = label.cuda()\n",
        "            result = model(data)\n",
        "            test_loss += criterion(result, label).item()\n",
        "            pred = result.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "    print('epoch {}, test loss {}, acc [{}/{}]'.format(epoch, test_loss, correct, len(test_loader.dataset)))\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('test_loss', test_loss, epoch)\n",
        "        writer.add_scalar('acc', correct/len(test_loader.dataset), epoch)"
      ],
      "metadata": {
        "id": "23c8vuiEtL08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNeXt model"
      ],
      "metadata": {
        "id": "mTx1X-Sd6o2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.process_time()\n",
        "root_path = '/'\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MyDataset('train', root_path=root_path), batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(MyDataset('test', root_path=root_path), batch_size=100)\n",
        "\n",
        "net = ResNeXt(10)\n",
        "\n",
        "net.cuda()\n",
        "optimizer = optim.Adam(net.parameters(), weight_decay=1e-5, betas=(0.9, 0.999))\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "log_path = './logs/'\n",
        "writer = SummaryWriter(log_path)\n",
        "\n",
        "epoch_num = 100\n",
        "lr0 = 1e-3\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    current_lr = lr0 / 2**int(epoch/50)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = current_lr\n",
        "    train_epoch(net, optimizer, train_loader, criterion, epoch, writer=writer)\n",
        "    test(net, test_loader, criterion, epoch, writer=writer)\n",
        "    if (epoch+1)%5==0:\n",
        "        torch.save(net.state_dict(), os.path.join('/ResNeXt_model_{}.pth'.format(epoch)))\n",
        "torch.save(net.state_dict(), os.path.join('/ResNeXt_model.pth'))\n",
        "\n",
        "print(\"Processing time:\", time.process_time() - tic)"
      ],
      "metadata": {
        "id": "oAwL3k3-I-vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SKNet model"
      ],
      "metadata": {
        "id": "J61z_Ulv6tNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.process_time()\n",
        "root_path = '/'\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MyDataset('train', root_path=root_path), batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(MyDataset('test', root_path=root_path), batch_size=100)\n",
        "\n",
        "net = SKNet(10)\n",
        "\n",
        "net.cuda()\n",
        "optimizer = optim.Adam(net.parameters(), weight_decay=1e-5, betas=(0.9, 0.999))\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "log_path = './logs/'\n",
        "writer = SummaryWriter(log_path)\n",
        "\n",
        "epoch_num = 100\n",
        "lr0 = 1e-3\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    current_lr = lr0 / 2**int(epoch/50)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = current_lr\n",
        "    train_epoch(net, optimizer, train_loader, criterion, epoch, writer=writer)\n",
        "    test(net, test_loader, criterion, epoch, writer=writer)\n",
        "    if (epoch+1)%5==0:\n",
        "        torch.save(net.state_dict(), os.path.join('/SKNet_model_{}.pth'.format(epoch)))\n",
        "torch.save(net.state_dict(), os.path.join('/SKNet_model.pth'))\n",
        "\n",
        "print(\"Processing time:\", time.process_time() - tic)"
      ],
      "metadata": {
        "id": "8tdPSpSk45_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
